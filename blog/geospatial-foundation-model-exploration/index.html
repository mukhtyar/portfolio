<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="description" property="og:description" content="Exploring where geospatial foundation models fit within the larger context of AI"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Exploring Geospatial Foundation Models</title><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="manifest" href="/site.webmanifest"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css?family=Work+Sans:400,500" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Spirax&text=ShrutiMukhtyar" rel="stylesheet"><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-PTG1RDVD45"></script><script type="module">window.dataLayer=window.dataLayer||[];function a(){dataLayer.push(arguments)}a("js",new Date);a("config","G-PTG1RDVD45");</script><link rel="stylesheet" href="/_astro/career-transition.CAvrcWe9.css">
<style>:root{--size-6: 1.75rem;--gray-5: #adb5bd;--size-4: 1.25rem;--size-content-3: 60ch;--font-size-6: 2.5rem}.blog-top-content[data-astro-cid-2q5oecfc]{margin-bottom:var(--size-6)}.blog-divider[data-astro-cid-2q5oecfc]{width:100%;height:1px;background-color:var(--gray-5);margin:var(--size-6) 0}.blog-content[data-astro-cid-2q5oecfc]{max-width:75%;margin:0 auto}.blog-content p{margin-top:var(--size-4)}.blog-content h2,.blog-content h3{margin-top:var(--size-6)}.blog-content li{max-inline-size:var(--size-content-3)}@media only screen and (max-width:1024px){h1[data-astro-cid-2q5oecfc]{margin:var(--size-4) 0;font-size:var(--font-size-6)}.blog-content[data-astro-cid-2q5oecfc]{max-width:100%}}
:root{--size-4: 1.25rem;--size-8: 3rem;--size-2: .5rem;--gray-9: #212529;--gray-7: #495057}nav.svelte-1jnx671{display:flex;align-items:baseline;padding:var(--size-4);justify-content:space-between}.brand.svelte-1jnx671{font-family:Spirax,serif;font-weight:400;font-size:var(--size-8);color:var(--brand);margin-inline:var(--size-2);text-transform:capitalize}.brand.svelte-1jnx671:hover{color:var(--grape-9)}ul.svelte-1jnx671{display:flex;align-items:center;gap:1rem;margin:0;list-style:none}li.svelte-1jnx671{display:inline-block;margin:0}a.svelte-1jnx671{font-size:1rem;line-height:1;padding:.25rem;outline:none;-webkit-text-decoration:none;text-decoration:none;color:var(--gray-9);text-transform:uppercase}a.active.svelte-1jnx671{-webkit-text-decoration:underline;text-decoration:underline}a.svelte-1jnx671:hover{color:var(--gray-7)}@media only screen and (max-width:768px){nav.svelte-1jnx671{flex-direction:column}ul.svelte-1jnx671{padding-inline-start:0;gap:.5rem;flex-wrap:wrap;margin-top:1rem}}:root{--size-8: 3rem;--size-2: .5rem}footer.svelte-1sr6y3t{width:100%;display:flex;flex-direction:column;padding:var(--size-8) var(--size-2);text-align:center;color:var(--grape-9)}a.svelte-1sr6y3t{color:var(--grape-9)}a.active.svelte-1sr6y3t{-webkit-text-decoration:underline;text-decoration:underline}a.svelte-1sr6y3t:hover,a.svelte-1sr6y3t:visited{color:var(--grape-9)}ul.svelte-1sr6y3t{list-style-type:none;display:flex;justify-content:center;padding:0;text-transform:uppercase}li.svelte-1sr6y3t{padding:var(--size-2)}
</style></head> <body> <style>astro-island,astro-slot,astro-static-slot{display:contents}</style><script>(()=>{var e=async t=>{await(await t())()};(self.Astro||(self.Astro={})).load=e;window.dispatchEvent(new Event("astro:load"));})();</script><script>(()=>{var A=Object.defineProperty;var g=(i,o,a)=>o in i?A(i,o,{enumerable:!0,configurable:!0,writable:!0,value:a}):i[o]=a;var d=(i,o,a)=>g(i,typeof o!="symbol"?o+"":o,a);{let i={0:t=>m(t),1:t=>a(t),2:t=>new RegExp(t),3:t=>new Date(t),4:t=>new Map(a(t)),5:t=>new Set(a(t)),6:t=>BigInt(t),7:t=>new URL(t),8:t=>new Uint8Array(t),9:t=>new Uint16Array(t),10:t=>new Uint32Array(t),11:t=>1/0*t},o=t=>{let[l,e]=t;return l in i?i[l](e):void 0},a=t=>t.map(o),m=t=>typeof t!="object"||t===null?t:Object.fromEntries(Object.entries(t).map(([l,e])=>[l,o(e)]));class y extends HTMLElement{constructor(){super(...arguments);d(this,"Component");d(this,"hydrator");d(this,"hydrate",async()=>{var b;if(!this.hydrator||!this.isConnected)return;let e=(b=this.parentElement)==null?void 0:b.closest("astro-island[ssr]");if(e){e.addEventListener("astro:hydrate",this.hydrate,{once:!0});return}let c=this.querySelectorAll("astro-slot"),n={},h=this.querySelectorAll("template[data-astro-template]");for(let r of h){let s=r.closest(this.tagName);s!=null&&s.isSameNode(this)&&(n[r.getAttribute("data-astro-template")||"default"]=r.innerHTML,r.remove())}for(let r of c){let s=r.closest(this.tagName);s!=null&&s.isSameNode(this)&&(n[r.getAttribute("name")||"default"]=r.innerHTML)}let p;try{p=this.hasAttribute("props")?m(JSON.parse(this.getAttribute("props"))):{}}catch(r){let s=this.getAttribute("component-url")||"<unknown>",v=this.getAttribute("component-export");throw v&&(s+=` (export ${v})`),console.error(`[hydrate] Error parsing props for component ${s}`,this.getAttribute("props"),r),r}let u;await this.hydrator(this)(this.Component,p,n,{client:this.getAttribute("client")}),this.removeAttribute("ssr"),this.dispatchEvent(new CustomEvent("astro:hydrate"))});d(this,"unmount",()=>{this.isConnected||this.dispatchEvent(new CustomEvent("astro:unmount"))})}disconnectedCallback(){document.removeEventListener("astro:after-swap",this.unmount),document.addEventListener("astro:after-swap",this.unmount,{once:!0})}connectedCallback(){if(!this.hasAttribute("await-children")||document.readyState==="interactive"||document.readyState==="complete")this.childrenConnectedCallback();else{let e=()=>{document.removeEventListener("DOMContentLoaded",e),c.disconnect(),this.childrenConnectedCallback()},c=new MutationObserver(()=>{var n;((n=this.lastChild)==null?void 0:n.nodeType)===Node.COMMENT_NODE&&this.lastChild.nodeValue==="astro:end"&&(this.lastChild.remove(),e())});c.observe(this,{childList:!0}),document.addEventListener("DOMContentLoaded",e)}}async childrenConnectedCallback(){let e=this.getAttribute("before-hydration-url");e&&await import(e),this.start()}async start(){let e=JSON.parse(this.getAttribute("opts")),c=this.getAttribute("client");if(Astro[c]===void 0){window.addEventListener(`astro:${c}`,()=>this.start(),{once:!0});return}try{await Astro[c](async()=>{let n=this.getAttribute("renderer-url"),[h,{default:p}]=await Promise.all([import(this.getAttribute("component-url")),n?import(n):()=>()=>{}]),u=this.getAttribute("component-export")||"default";if(!u.includes("."))this.Component=h[u];else{this.Component=h;for(let f of u.split("."))this.Component=this.Component[f]}return this.hydrator=p,this.hydrate},e,this)}catch(n){console.error(`[astro-island] Error hydrating ${this.getAttribute("component-url")}`,n)}}attributeChangedCallback(){this.hydrate()}}d(y,"observedAttributes",["props"]),customElements.get("astro-island")||customElements.define("astro-island",y)}})();</script><astro-island uid="2edj0q" component-url="/_astro/Nav.CrzuLXTU.js" component-export="default" renderer-url="/_astro/client.svelte.DK5xu9hA.js" props="{&quot;routes&quot;:[1,[[0,{&quot;path&quot;:[0,&quot;/projects&quot;],&quot;label&quot;:[0,&quot;Projects&quot;]}],[0,{&quot;path&quot;:[0,&quot;https://www.linkedin.com/in/shruti-mukhtyar/&quot;],&quot;label&quot;:[0,&quot;LinkedIn&quot;],&quot;icon&quot;:[0,null]}],[0,{&quot;path&quot;:[0,&quot;https://github.com/mukhtyar&quot;],&quot;label&quot;:[0,&quot;Github&quot;],&quot;icon&quot;:[0,null]}],[0,{&quot;path&quot;:[0,&quot;https://observablehq.com/@mukhtyar?type=collections&quot;],&quot;label&quot;:[0,&quot;ObservableHQ&quot;],&quot;icon&quot;:[0,null]}],[0,{&quot;path&quot;:[0,&quot;https://www.kaggle.com/shrutimukhtyar&quot;],&quot;label&quot;:[0,&quot;Kaggle&quot;],&quot;icon&quot;:[0,null]}]]],&quot;currentRoute&quot;:[0,&quot;/blog/geospatial-foundation-model-exploration&quot;]}" ssr client="load" opts="{&quot;name&quot;:&quot;Nav&quot;,&quot;value&quot;:true}" await-children><!--[--><nav class="svelte-1jnx671"><div><a class="brand svelte-1jnx671" href="/">Shruti Mukhtyar</a></div> <ul role="menu" class="svelte-1jnx671"><!--[--><li class="svelte-1jnx671"><a aria-current="false" href="/projects" class="svelte-1jnx671"><!--[!-->Projects<!--]--></a></li><li class="svelte-1jnx671"><a aria-current="false" href="https://www.linkedin.com/in/shruti-mukhtyar/" class="svelte-1jnx671"><!--[--><!----><svg width="30" height="30" fill="currentColor" aria-hidden="true" aria-labelledby="false" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><!--[!--><!--]--><path d="M26.2,4H5.8C4.8,4,4,4.8,4,5.7v20.5c0,0.9,0.8,1.7,1.8,1.7h20.4c1,0,1.8-0.8,1.8-1.7V5.7C28,4.8,27.2,4,26.2,4z M11.1,24.4 H7.6V13h3.5V24.4z M9.4,11.4c-1.1,0-2.1-0.9-2.1-2.1c0-1.2,0.9-2.1,2.1-2.1c1.1,0,2.1,0.9,2.1,2.1S10.5,11.4,9.4,11.4z M24.5,24.3 H21v-5.6c0-1.3,0-3.1-1.9-3.1c-1.9,0-2.1,1.5-2.1,2.9v5.7h-3.5V13h3.3v1.5h0.1c0.5-0.9,1.7-1.9,3.4-1.9c3.6,0,4.3,2.4,4.3,5.5V24.3z"></path></svg><!----> <span class="sr-only">LinkedIn</span><!--]--></a></li><li class="svelte-1jnx671"><a aria-current="false" href="https://github.com/mukhtyar" class="svelte-1jnx671"><!--[--><!----><svg width="30" height="30" fill="currentColor" aria-hidden="true" aria-labelledby="false" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><!--[!--><!--]--><path d="M16,2a14,14,0,0,0-4.43,27.28c.7.13,1-.3,1-.67s0-1.21,0-2.38c-3.89.84-4.71-1.88-4.71-1.88A3.71,3.71,0,0,0,6.24,22.3c-1.27-.86.1-.85.1-.85A2.94,2.94,0,0,1,8.48,22.9a3,3,0,0,0,4.08,1.16,2.93,2.93,0,0,1,.88-1.87c-3.1-.36-6.37-1.56-6.37-6.92a5.4,5.4,0,0,1,1.44-3.76,5,5,0,0,1,.14-3.7s1.17-.38,3.85,1.43a13.3,13.3,0,0,1,7,0c2.67-1.81,3.84-1.43,3.84-1.43a5,5,0,0,1,.14,3.7,5.4,5.4,0,0,1,1.44,3.76c0,5.38-3.27,6.56-6.39,6.91a3.33,3.33,0,0,1,.95,2.59c0,1.87,0,3.38,0,3.84s.25.81,1,.67A14,14,0,0,0,16,2Z"></path></svg><!----> <span class="sr-only">Github</span><!--]--></a></li><li class="svelte-1jnx671"><a aria-current="false" href="https://observablehq.com/@mukhtyar?type=collections" class="svelte-1jnx671"><!--[--><!----><svg width="30" height="30" fill="currentColor" aria-hidden="true" aria-labelledby="false" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 26 28"><!--[!--><!--]--><path d="M12.5 22.6667C11.3458 22.6667 10.3458 22.4153 9.5 21.9127C8.65721 21.412 7.98339 20.7027 7.55521 19.8654C7.09997 18.9942 6.76672 18.0729 6.56354 17.1239C6.34796 16.0947 6.24294 15.0483 6.25 14C6.25 13.1699 6.30417 12.3764 6.41354 11.6176C6.52188 10.8598 6.72292 10.0894 7.01563 9.30748C7.30833 8.52555 7.68542 7.84763 8.14479 7.27274C8.62304 6.68378 9.24141 6.20438 9.95208 5.87163C10.6979 5.51244 11.5458 5.33333 12.5 5.33333C13.6542 5.33333 14.6542 5.58467 15.5 6.08733C16.3428 6.588 17.0166 7.29733 17.4448 8.13459C17.8969 8.99644 18.2271 9.9103 18.4365 10.8761C18.6448 11.841 18.75 12.883 18.75 14C18.75 14.8301 18.6958 15.6236 18.5865 16.3824C18.4699 17.1702 18.2639 17.9446 17.9719 18.6925C17.6698 19.4744 17.2948 20.1524 16.8427 20.7273C16.3906 21.3021 15.7927 21.7692 15.0479 22.1284C14.3031 22.4876 13.4542 22.6667 12.5 22.6667ZM14.7063 16.2945C15.304 15.6944 15.6365 14.864 15.625 14C15.625 13.1073 15.326 12.3425 14.7292 11.7055C14.1313 11.0685 13.3885 10.75 12.5 10.75C11.6115 10.75 10.8688 11.0685 10.2708 11.7055C9.68532 12.3123 9.36198 13.1405 9.375 14C9.375 14.8927 9.67396 15.6575 10.2708 16.2945C10.8688 16.9315 11.6115 17.25 12.5 17.25C13.3885 17.25 14.124 16.9315 14.7063 16.2945ZM12.5 27C19.4031 27 25 21.1792 25 14C25 6.82075 19.4031 1 12.5 1C5.59687 1 0 6.82075 0 14C0 21.1792 5.59687 27 12.5 27Z"></path></svg><!----> <span class="sr-only">ObservableHQ</span><!--]--></a></li><li class="svelte-1jnx671"><a aria-current="false" href="https://www.kaggle.com/shrutimukhtyar" class="svelte-1jnx671"><!--[--><!----><svg width="30" height="30" fill="currentColor" aria-hidden="true" aria-labelledby="false" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--[!--><!--]--><path d="M385.708,476.478L254.742,313.713l125.578-121.534c2.334-2.426,1.526-9.433-4.761-9.433h-62.16    c-3.145,0-6.288,1.618-9.433,4.761L185.128,307.604V32.738c0-4.491-2.247-6.737-6.738-6.737h-46.618    c-4.492,0-6.737,2.246-6.737,6.737v446.433c0,4.491,2.246,6.738,6.737,6.738h46.618c4.491,0,6.738-2.247,6.738-6.738v-97.91    l27.666-26.317l99.257,126.294c2.695,3.145,5.839,4.762,9.432,4.762h60.095c3.143,0,4.939-0.899,5.389-2.696L385.708,476.478z"></path></svg><!----> <span class="sr-only">Kaggle</span><!--]--></a></li><!--]--></ul></nav><!--]--><!--astro:end--></astro-island> <main id="main">  <section data-astro-cid-2q5oecfc> <div class="container" data-astro-cid-2q5oecfc> <div class="blog-top-content" data-astro-cid-2q5oecfc> <h1 data-astro-cid-2q5oecfc>Exploring Geospatial Foundation Models</h1> <p data-astro-cid-2q5oecfc> Friday, August 23, 2024 </p> </div> <div class="blog-divider" data-astro-cid-2q5oecfc></div> <div class="blog-content" data-astro-cid-2q5oecfc> <p>Artificial intelligence (AI) research is transforming how we work with geospatial data. As part of a group capstone project during my <a href="https://climatebase.org/fellowship">Climatebase Fellowship</a>, I explored several papers on geospatial foundation models and their applications in tackling climate challenges. A list of papers we read in inlcuded at the end of this blog post.</p>
<p>I wrote this post to better understand where geospatial foundation models fit within the larger context of AI.</p>
<h2 id="understanding-key-ai-terminology">Understanding Key AI Terminology</h2>
<p>To navigate the fast-moving AI space, here are some key terms according to <a href="https://cset.georgetown.edu/article/what-are-generative-ai-large-language-models-and-foundation-models/">Georgetownâ€™s Center for Security and Emerging Technology</a>:</p>
<ul>
<li>
<p><strong>Generative AI</strong>: This is a broad term used for AI systems that create content, unlike those that classify or group data or choose actions (e.g. to steer autonomous devices/cars). Examples include image generators (Midjourney), large language models (GPT-4), code tools (Copilot), and audio generators (VALL-E).</p>
</li>
<li>
<p><strong>Large Language Models (LLM)</strong>: These AI systems work with language and are trained on large text datasets. Examples include OpenAIâ€™s GPT-4, Googleâ€™s PaLM, and Metaâ€™s LLaMA. A product like OpenAIâ€™s ChatGPT is created by fine-tuning a LLM like GPT-4 for question-answering &#x26; other tasks.</p>
</li>
<li>
<p><strong>Foundation Models</strong>: Unlike AI systems designed for a single purpose, foundation models are base models trained on large datasets that can be adapted for a wide range of downstream tasks. The term foundation models was popularized by a <a href="https://crfm.stanford.edu/report.html">group</a> at Stanford University. A LLM is an example of a foundation model.</p>
</li>
</ul>
<h2 id="geospatial-foundation-models">Geospatial Foundation Models</h2>
<p>Geospatial foundation models are large deep learning neural networks trained on geospatial data from satellites, aerial imagery, and other sensors. These models can be fine-tuned for specific tasks like identifying and classifying crop types or land cover, <a href="https://www.sciencedirect.com/science/article/pii/S003442572300439X">measuring forest canopy</a>, <a href="https://arxiv.org/abs/2403.06860">predicting desert locust swarms</a>, and more.</p>
<h3 id="model-architecture">Model architecture</h3>
<p>Geospatial foundation models evolved from breakthroughs in deep learning in computer vision. While reading about on geospatial foundation models, two model architectures that frequently appear are Convolution Neural Networks (CNN) and ViT (Vision Transformer). Most of the modern geospatial foundation models are based on a ViT architecture. For a detailed comparison between the two model architectures see this <a href="https://tobiasvanderwerff.github.io/2024/05/15/cnn-vs-vit.html">article</a>. Below are some highlights:</p>
<ul>
<li>
<p>CNN</p>
<ul>
<li>The model uses convolutional layers to automatically extract features from input images by applying filters (kernels) that detect patterns (edges, textures, objects). As data passes through multiple convolutional layers, the neural network learns to recognize increasingly complex patterns.</li>
<li>Model has built-in assumptions (inductive biases) like nearby pixels are more related than distant pixels (spatial locality) and objects in an image can appear anywhere (translation invariance).</li>
<li>Model is trained using a supervised learning approach using labeled datasets.
Generally requires less data to train effectively due to inductive biases and performs well on small to medium-sized datasets.</li>
<li>Useful in scenarios where training data is limited</li>
</ul>
</li>
<li>
<p>ViT</p>
<ul>
<li>The model applies the transformer architecture (used in LLMs) to images. Images are divided into patches, each patch is embedded and these embeddings are processed using self-attention mechanisms.</li>
<li>Model has very few built-in assumptions (inductive biases). All pixels are related to other pixels with equal weight.</li>
<li>Model is typically trained using self-supervision techniques using require much larger datasets.</li>
<li>Excels on large-scale datasets using self-attention mechanisms to learn global features. Without large datasets, ViTs can underperform compared to CNNs unless pretrained models are used.</li>
<li>Useful in tasks that require global context or where large amounts of data are available (e.g., medical imaging, large-scale image recognition). Also used in multimodal tasks that involve both vision and language.</li>
</ul>
</li>
</ul>
<h3 id="examples">Examples</h3>
<p>The following are some examples of geospatial foundation models described in the readings:</p>
<ul>
<li>
<p>Prithvi</p>
<ul>
<li>Open source model created by NASA and IBM</li>
<li>Trained on a huge dataset of Landsat &#x26; Sentinel imagery</li>
<li>Model architecture consists of a self-supervised encoder with a ViT and Masked AutoEncoder (MAE) learning strategy with an MSE loss function.</li>
<li>Described as a Swiss Army knife for geospatial professionals, capable of analyzing everything from forest cover to urban sprawl.</li>
</ul>
</li>
<li>
<p>DINOv2</p>
<ul>
<li>Open source model created by Meta</li>
<li>Trained on private high-resolution satellite images from Maxar to produce a global canopy height map</li>
<li>Model architecture is based on a ViT combined with a self-supervised learning approach and a student-teacher framework.</li>
<li>Potential use in improving the monitoring of forest degradation/restoration, and forest carbon dynamics.</li>
</ul>
</li>
<li>
<p>SkySense</p>
<ul>
<li>Open source model created by research scientists</li>
<li>Trained on multiple types of satellite dataâ€”optical, radar, and multispectral imagery</li>
<li>Model architecture is based on a ViT</li>
<li>Ability to interpret various data sources makes it useful to tackle variety of geospatial applications</li>
</ul>
</li>
</ul>
<h3 id="challenges">Challenges</h3>
<p>Geospatial models are large but not as large as the largest LLMs. While geospatial foundation models share many similarities with computer vision models, they also face unique challenges:</p>
<ul>
<li>Handling varied resolutions (spatial resolution of images, time intervals, etc.) and data formats from different sensors (optical imagery, infrared, radar, LIDAR, and multi-spectral data).</li>
<li>Processing large-scale geospatial data often requires high computational resources and specialized tools.</li>
</ul>
<p>Geospatial foundation models offer tremendous potential in addressing climate challenges. I hope to continue learning and exploring the exciting intersection of geospatial data and AI in future posts.</p>
<br>
<h4 id="reading-list">Reading list:</h4>
<br>
<ul>
<li><a href="https://www.sciencedirect.com/science/article/pii/S0034425723004704">Need and vision for global medium-resolution Landsat and Sentinel-2 data products</a></li>
<li><a href="https://arxiv.org/abs/2402.01444">Mission Critical â€” Satellite Data is a Distinct Modality in Machine Learning</a></li>
<li><a href="https://arxiv.org/abs/2310.18660">FMs for Generalist Geospatial Artificial Intelligence</a></li>
<li><a href="https://www.sciencedirect.com/science/article/pii/S003442572300439X">Very high resolution canopy height maps from RGB imagery using self-supervised vision transformer and convolutional decoder trained on aerial lidar</a></li>
<li><a href="https://arxiv.org/abs/2312.10115">SkySense: A Multi-Modal Remote Sensing FM Towards Universal Interpretation for Earth Observation Imagery</a></li>
<li><a href="https://arxiv.org/abs/2403.06860">A Geospatial Approach to Predicting Desert Locust Breeding Grounds in Africa</a></li>
<li><a href="https://arxiv.org/abs/2402.01749">Towards Urban General Intelligence: A Review and Outlook of Urban FMs</a></li>
<li><a href="https://arxiv.org/abs/2401.04464">PhilEO Bench: Evaluating Geo-Spatial FMs</a></li>
</ul> </div> </div> </section>  </main> <!--[--><footer class="svelte-1sr6y3t"><ul role="menu" class="svelte-1sr6y3t"><li class="svelte-1sr6y3t"><a aria-current="false" href="/" class="svelte-1sr6y3t">Home</a></li> <!--[--><!--[--><li class="svelte-1sr6y3t"><a aria-current="false" href="/projects" class="svelte-1sr6y3t">Projects</a></li><!--]--><!--[!--><!--]--><!--[!--><!--]--><!--[!--><!--]--><!--[!--><!--]--><!--]--></ul> <div>ðŸš€ Built with <a href="https://astro.build/" target="_blank" class="svelte-1sr6y3t">Astro</a>, <a href="https://svelte.dev/" target="_blank" class="svelte-1sr6y3t">Svelte</a>, <a href="https://open-props.style" target="_blank" class="svelte-1sr6y3t">Open Props</a> &amp; <a href="https://code.claude.com/docs" target="_blank" class="svelte-1sr6y3t">Claude</a></div></footer><!--]--> </body></html> 